{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTCtLdlR_uyp"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# os.chdir(\"/content/gdrive/My Drive/Ece 9039/dataset\")"
      ],
      "metadata": {
        "id": "QyRqTkivC9OT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, classification_report,confusion_matrix\n",
        "import cv2\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.utils import image_dataset_from_directory\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization, GlobalAveragePooling2D\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "from keras.utils import to_categorical, plot_model\n",
        "from keras.applications import DenseNet121\n",
        "from keras.applications import VGG16\n",
        "from keras.applications.resnet import ResNet152\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping"
      ],
      "metadata": {
        "id": "PAp9UaEMAPxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display image sample from dataset\n",
        "fig, ax = plt.subplots(1,4, figsize=(15,8))\n",
        "# Cataract Image\n",
        "cataract_img = plt.imread('./cataract/_1_5346540.jpg')\n",
        "ax[0].imshow(cataract_img)\n",
        "ax[0].set_title('cataract, Resolution: {:}'.format(cataract_img.shape))\n",
        "\n",
        "# Diabetic_retinopathy\n",
        "diabetic_retinopathy_img = plt.imread('./diabetic_retinopathy/100_left.jpeg')\n",
        "ax[1].imshow(diabetic_retinopathy_img)\n",
        "ax[1].set_title('diabetic_retinopathy, Resolution: {:}'.format(diabetic_retinopathy_img.shape))\n",
        "\n",
        "# Glaucoma Image\n",
        "glaucoma_img = plt.imread('./glaucoma/_0_4517448.jpg')\n",
        "ax[2].imshow(glaucoma_img)\n",
        "ax[2].set_title('glaucoma, Resolution: {:}'.format(glaucoma_img.shape))\n",
        "\n",
        "# Normal Image\n",
        "normal_img = plt.imread('./normal/8_left.jpg')\n",
        "ax[3].imshow(normal_img)\n",
        "ax[3].set_title('normal, Resolution: {:}'.format(normal_img.shape))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6WoFjyquAZYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove noise image\n",
        "def remove_corrupt(path):\n",
        "    corrupt = 0\n",
        "    for folder_name in ('glaucoma', 'normal', 'cataract', 'diabetic_retinopathy'):\n",
        "        folder_path = os.path.join(path, folder_name)\n",
        "        for fname in os.listdir(folder_path):\n",
        "            fpath = os.path.join(folder_path, fname)\n",
        "            try:\n",
        "                fobj = open(fpath, \"rb\")\n",
        "                is_jfif = tf.compat.as_bytes(\"JFIF\") in fobj.peek(10)\n",
        "            finally:\n",
        "                fobj.close()\n",
        "            if not is_jfif:\n",
        "                corrupt += 1\n",
        "                # Drop corrupted image\n",
        "                os.remove(fpath)\n",
        "    print(f'Total %d corrupt images dataset: ' % corrupt)\n",
        "    print('All corrupt images dropped.' + '\\n')\n",
        "\n",
        "remove_corrupt_info = remove_corrupt('./')"
      ],
      "metadata": {
        "id": "4kHBXvBMBl_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display all contents of directory\n",
        "dataset_path = os.listdir('./')\n",
        "print (dataset_path)\n",
        "print(\"Types of classes labels found: \", len(dataset_path))\n",
        "\n",
        "def dataset_info(path_dir):\n",
        "  label_iamge_info = []\n",
        "\n",
        "  for item in path_dir: \n",
        "    all_image = os.listdir(item+'/')\n",
        "  # Add them to the list\n",
        "    \n",
        "    label_iamge_info.append((item, str(len(all_image))))\n",
        "  return label_iamge_info\n",
        "\n",
        "label_iamge_info = dataset_info(dataset_path)\n",
        "\n",
        "# print out the label images information\n",
        "df = pd.DataFrame(data=label_iamge_info, columns=['Labels', 'image_number'])\n",
        "df.head()"
      ],
      "metadata": {
        "id": "QKln-FZQCp8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split train dataset\n",
        "train_data = image_dataset_from_directory('./',\n",
        "                                        seed = 2022, \n",
        "                                        image_size=(224, 224),\n",
        "                                        batch_size = 64,\n",
        "                                        shuffle=True,\n",
        "                                        color_mode = 'rgb',\n",
        "                                        label_mode = 'categorical',\n",
        "                                        validation_split=0.3, \n",
        "                                        subset='training')\n",
        "\n",
        "# split validation dataset\n",
        "validate_data = image_dataset_from_directory('./',\n",
        "                                        seed = 2022, \n",
        "                                        image_size=(224, 224),\n",
        "                                        batch_size = 64,\n",
        "                                        shuffle=True,\n",
        "                                        color_mode = 'rgb',\n",
        "                                        label_mode = 'categorical',\n",
        "                                        validation_split=0.3, \n",
        "                                        subset='validation')\n",
        "\n",
        "# test train split\n",
        "val_batches = tf.data.experimental.cardinality(validate_data)\n",
        "test_data = validate_data.take((2*val_batches) // 3)\n",
        "validate_data = validate_data.skip((2*val_batches) // 3)\n",
        "\n"
      ],
      "metadata": {
        "id": "gONRdVoTDUB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scale image 0.0 to 1.0\n",
        "train_data = train_data.map(lambda x, y: (x/255, y))\n",
        "validate_data = validate_data.map(lambda x, y: (x/255, y))\n",
        "test_data = test_data.map(lambda x, y: (x/255, y))\n",
        "\n",
        "print(f\"# train batchs = {len(train_data)}, # validate batchs = {len(validate_data)}, # test batch = {len(test_data)}\")"
      ],
      "metadata": {
        "id": "Faf-8eavD0ga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this part is for tuning function\n",
        "# set the image dimensions and batch size\n",
        "img_width, img_height = 224, 224\n",
        "batch_size = 64\n",
        "# create the image data generators for training and validation data\n",
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        validation_split=0.3)"
      ],
      "metadata": {
        "id": "nNGsueGrbwp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DenseNet121 hyperparameter tuning function\n",
        "def Grid_Search_TL_model(active = 'relu', learning_rate = 0.01, dropout = 0.5):\n",
        "  densenet = DenseNet121(weights = \"imagenet\", include_top = False, input_shape=(224,224,3), pooling='avg')\n",
        "\n",
        "  model = Sequential([\n",
        "      densenet,\n",
        "      # add hidden layers after tuning\n",
        "      Dense(512, activation=active),\n",
        "      Dropout(dropout),\n",
        "      Dense(256, activation=active),\n",
        "      Dropout(dropout),\n",
        "      Dense(4, activation='sigmoid')\n",
        "  ])\n",
        "   # Freeze the layers of the pre-trained model\n",
        "  densenet.trainable = False\n",
        "\n",
        "  adam = Adam(learning_rate=learning_rate)\n",
        "  model.compile(optimizer = adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "  #gridsearch on 2 activation function, 2 dropout rate and 3 learning_rate\n",
        "param_grid = dict(active=['relu', 'tanh'],learning_rate = [0.01,0.001,0.0001], dropout = [0.5,0.8])\n",
        "\n",
        "hp_model = KerasClassifier(build_fn=Grid_Search_TL_model, verbose=0)\n",
        "#instantiate gridsearch object using 3 fold crossvaliadtion\n",
        "grid = GridSearchCV(estimator=hp_model, param_grid=param_grid, cv=3,error_score='raise')\n",
        "\n",
        "# get the training and validation data generators and their corresponding targets\n",
        "X_train, y_train = train_datagen.flow_from_directory('./', subset='training', target_size=(img_width, img_height), batch_size=batch_size, class_mode='categorical').next()\n",
        "X_validate, y_validate = train_datagen.flow_from_directory('./', subset='validation', target_size=(img_width, img_height), batch_size=batch_size, class_mode='categorical').next()\n",
        "\n",
        "grid_result = grid.fit(X_train, y_train, validation_data=(X_validate, y_validate), epochs = 1, callbacks=[EarlyStopping(patience=3)])\n",
        "\n",
        "#determine the best parameter\n",
        "print(grid_result.best_params_)\n",
        "print(grid_result.best_score_)"
      ],
      "metadata": {
        "id": "Wo1kczFYb5vk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "densenet = DenseNet121(weights = \"imagenet\", include_top = False, input_shape=(224,224,3), pooling='avg')\n",
        "\n",
        "# Freeze the layers of the pre-trained model\n",
        "for layer in densenet.layers:\n",
        "      layer.trainable = False\n",
        "\n",
        "# densenet.summary()\n",
        "\n",
        "# Create the model\n",
        "model = Sequential([\n",
        "    densenet,\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(4, activation='sigmoid')\n",
        "])\n",
        "adam = Adam(learning_rate=0.001)\n",
        "model.compile(optimizer= adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# plot of DenseNet121 model\n",
        "plot_model(model, to_file='DenseNet121-plant.png', show_shapes=True)\n"
      ],
      "metadata": {
        "id": "87M4xXq-EV2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pick loss, val_loss, accuracy and val_accuracy\n",
        "hist = model.fit(train_data, validation_data = validate_data, epochs = 20, verbose = 1, batch_size=64)\n",
        "train_loss = hist.history['loss']\n",
        "val_loss = hist.history['val_loss']\n",
        "train_accuracy = hist.history['accuracy']\n",
        "val_accuracy = hist.history['val_accuracy']\n",
        "\n",
        "# print loss, val_loss, accuracy and val_accuracy\n",
        "print(train_loss)\n",
        "print(train_accuracy)\n",
        "print(val_loss)\n",
        "print(val_accuracy)\n",
        "\n",
        "# loss for each iteration, and make a plot of iterations/epochs vs loss\n",
        "\n",
        "epochs = list(range(1,21))\n",
        "plt.figure(figsize=(9,3))\n",
        "plt.plot(epochs,train_loss, color='blue', label='train loss')\n",
        "plt.plot(epochs,val_loss, color='orange', label='validation loss')\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.title(\"Loss vs Epoch (DenseNet121)\")\n",
        "plt.legend(loc='best')\n",
        "plt.show()\n",
        "\n",
        "# accuracy for each iteration, and make a plot of iterations/epochs vs accuracy\n",
        "plt.figure(figsize=(9,3))\n",
        "plt.plot(epochs,train_accuracy, color='blue', label='train accuracy')\n",
        "plt.plot(epochs,val_accuracy, color='orange', label='validation accuracy')\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.title('Accuracy vs Epoch (DenseNet121)')\n",
        "plt.legend(loc='best')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "WMLWz5HaE1Gm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print out model accuracy\n",
        "loss, accuracy = model.evaluate(test_data, verbose=0)\n",
        "print('Test accuracy: %.2f%%' % (accuracy * 100))"
      ],
      "metadata": {
        "id": "Wcb9AZSEKPA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "real_value = []\n",
        "pred_value = []\n",
        "\n",
        "labels = dict()\n",
        "\n",
        "# perdict each batch of images \n",
        "for images_batch, labels_batch in test_data:\n",
        "    for i in range(images_batch.shape[0]):\n",
        "        image = images_batch[i].numpy().astype('uint8')\n",
        "        label = labels_batch[i].numpy()\n",
        "        \n",
        "        real_value.append(np.argmax(label, axis=0))\n",
        "    \n",
        "        batch_prediction = model.predict(images_batch, verbose=0)\n",
        "\n",
        "        pred_value.append(np.argmax(batch_prediction[i]))\n",
        "\n",
        "real_value1 = []\n",
        "pred_value1 = []\n",
        "\n",
        "# convert perdict reult for future scores calculate\n",
        "for i in real_value:\n",
        "  if i == 0:\n",
        "    real_value1.append([1,0,0,0])\n",
        "  elif i == 1:\n",
        "    real_value1.append([0,1,0,0])\n",
        "  elif i == 2:\n",
        "    real_value1.append([0,0,1,0])\n",
        "  elif i == 3:\n",
        "    real_value1.append([0,0,0,1])\n",
        "\n",
        "for i in pred_value:\n",
        "  if i == 0:\n",
        "    pred_value1.append([1,0,0,0])\n",
        "  elif i == 1:\n",
        "    pred_value1.append([0,1,0,0])\n",
        "  elif i == 2:\n",
        "    pred_value1.append([0,0,1,0])\n",
        "  elif i == 3:\n",
        "    pred_value1.append([0,0,0,1])\n",
        "\n",
        "\n",
        "# calculate precision, recall and f1 socres and print\n",
        "precision = precision_score(real_value1, pred_value1, average='macro')\n",
        "recall = recall_score(real_value1, pred_value1, average='macro')\n",
        "f1 = f1_score(real_value1, pred_value1, average='macro')\n",
        "\n",
        "print('Precision: {:.3f}'.format(precision))\n",
        "print('Recall: {:.3f}'.format(recall))\n",
        "print('F1 score: {:.3f}'.format(f1))"
      ],
      "metadata": {
        "id": "TyCPF-viVYti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VGG16 hyperparameter tuning function\n",
        "def Grid_Search_TL_model(active = 'relu', learning_rate = 0.01, dropout = 0.5):\n",
        "  vgg16 = VGG16(weights = \"imagenet\", include_top = False, input_shape=(224,224,3), pooling='avg')\n",
        "\n",
        "  model = Sequential([\n",
        "      vgg16,\n",
        "      # add hidden layers after tuning\n",
        "      Dense(512, activation=active),\n",
        "      Dropout(dropout),\n",
        "      Dense(128, activation=active),\n",
        "      Dropout(dropout),\n",
        "      Dense(4, activation='sigmoid')\n",
        "  ])\n",
        "   # Freeze the layers of the pre-trained model\n",
        "  for layer in vgg16.layers:\n",
        "      layer.trainable = False\n",
        "\n",
        "  adam = Adam(learning_rate=learning_rate)\n",
        "  model.compile(optimizer = adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "  #gridsearch on 2 activation function, 2 dropout rate and 3 learning_rate\n",
        "param_grid = dict(active=['relu', 'tanh'],learning_rate = [0.01,0.001,0.0001], dropout = [0.5,0.8])\n",
        "\n",
        "hp_model = KerasClassifier(build_fn=Grid_Search_TL_model, verbose=0)\n",
        "#instantiate gridsearch object using 3 fold crossvaliadtion\n",
        "grid = GridSearchCV(estimator=hp_model, param_grid=param_grid, cv=3,error_score='raise')\n",
        "\n",
        "# get the training and validation data generators and their corresponding targets\n",
        "X_train, y_train = train_datagen.flow_from_directory('./', subset='training', target_size=(img_width, img_height), batch_size=batch_size, class_mode='categorical').next()\n",
        "X_validate, y_validate = train_datagen.flow_from_directory('./', subset='validation', target_size=(img_width, img_height), batch_size=batch_size, class_mode='categorical').next()\n",
        "\n",
        "grid_result = grid.fit(X_train, y_train, validation_data=(X_validate, y_validate), epochs = 1, callbacks=[EarlyStopping(patience=3)])\n",
        "\n",
        "#determine the best parameter\n",
        "print(grid_result.best_params_)\n",
        "print(grid_result.best_score_)"
      ],
      "metadata": {
        "id": "2WgXWZpxL2bL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg16 = VGG16(weights = \"imagenet\", include_top = False, input_shape=(224,224,3), pooling='avg')\n",
        "\n",
        "# Freeze the layers of the pre-trained model\n",
        "for layer in vgg16.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# vgg16.summary()\n",
        "\n",
        "# Create the model\n",
        "model = Sequential([\n",
        "    vgg16,\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(4, activation='sigmoid')\n",
        "])\n",
        "adam = Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# plot of VGG16 model\n",
        "plot_model(model, to_file='VGG16-plant.png', show_shapes=True)"
      ],
      "metadata": {
        "id": "d9ElUhQtQXIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pick loss, val_loss, accuracy and val_accuracy for comparsion\n",
        "hist = model.fit(train_data, validation_data = validate_data, epochs = 20, verbose = 1, batch_size=64)\n",
        "train_loss = hist.history['loss']\n",
        "val_loss = hist.history['val_loss']\n",
        "train_accuracy = hist.history['accuracy']\n",
        "val_accuracy = hist.history['val_accuracy']\n",
        "\n",
        "# print loss, val_loss, accuracy and val_accuracy\n",
        "print(train_loss)\n",
        "print(train_accuracy)\n",
        "print(val_loss)\n",
        "print(val_accuracy)\n",
        "\n",
        "# loss for each iteration, and make a plot of iterations/epochs vs loss\n",
        "\n",
        "epochs = list(range(1,21))\n",
        "plt.figure(figsize=(9,3))\n",
        "plt.plot(epochs,train_loss, color='blue', label='train loss')\n",
        "plt.plot(epochs,val_loss, color='orange', label='validation loss')\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.title(\"Loss vs Epoch (VGG16)\")\n",
        "plt.legend(loc='best')\n",
        "plt.show()\n",
        "\n",
        "# accuracy for each iteration, and make a plot of iterations/epochs vs accuracy\n",
        "plt.figure(figsize=(9,3))\n",
        "plt.plot(epochs,train_accuracy, color='blue', label='train accuracy')\n",
        "plt.plot(epochs,val_accuracy, color='orange', label='validation accuracy')\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.title('Accuracy vs Epoch (VGG16)')\n",
        "plt.legend(loc='best')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7GT0_7Y6QkLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print out model accuracy\n",
        "loss, accuracy = model.evaluate(test_data, verbose=0)\n",
        "print('Test accuracy: %.2f%%' % (accuracy * 100))"
      ],
      "metadata": {
        "id": "7MOebY9pQpOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "real_value = []\n",
        "pred_value = []\n",
        "\n",
        "\n",
        "\n",
        "labels = dict()\n",
        "\n",
        "# perdict each batch of images \n",
        "for images_batch, labels_batch in test_data:\n",
        "    for i in range(images_batch.shape[0]):\n",
        "        image = images_batch[i].numpy().astype('uint8')\n",
        "        label = labels_batch[i].numpy()\n",
        "        \n",
        "        real_value.append(np.argmax(label, axis=0))\n",
        "    \n",
        "        batch_prediction = model.predict(images_batch, verbose=0)\n",
        "\n",
        "        pred_value.append(np.argmax(batch_prediction[i]))\n",
        "\n",
        "real_value1 = []\n",
        "pred_value1 = []\n",
        "\n",
        "# convert perdict reult for future scores calculate\n",
        "for i in real_value:\n",
        "  if i == 0:\n",
        "    real_value1.append([1,0,0,0])\n",
        "  elif i == 1:\n",
        "    real_value1.append([0,1,0,0])\n",
        "  elif i == 2:\n",
        "    real_value1.append([0,0,1,0])\n",
        "  elif i == 3:\n",
        "    real_value1.append([0,0,0,1])\n",
        "\n",
        "for i in pred_value:\n",
        "  if i == 0:\n",
        "    pred_value1.append([1,0,0,0])\n",
        "  elif i == 1:\n",
        "    pred_value1.append([0,1,0,0])\n",
        "  elif i == 2:\n",
        "    pred_value1.append([0,0,1,0])\n",
        "  elif i == 3:\n",
        "    pred_value1.append([0,0,0,1])\n",
        "\n",
        "\n",
        "# calculate precision, recall and f1 socres and print\n",
        "precision = precision_score(real_value1, pred_value1, average='macro')\n",
        "recall = recall_score(real_value1, pred_value1, average='macro')\n",
        "f1 = f1_score(real_value1, pred_value1, average='macro')\n",
        "\n",
        "print('Precision: {:.3f}'.format(precision))\n",
        "print('Recall: {:.3f}'.format(recall))\n",
        "print('F1 score: {:.3f}'.format(f1))"
      ],
      "metadata": {
        "id": "vs7CBfRCVUzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ResNet152 hyperparameter tuning function\n",
        "def Grid_Search_TL_model(active = 'relu', learning_rate = 0.01, dropout = 0.5):\n",
        "  resnet152 = ResNet152(weights = \"imagenet\", include_top = False, input_shape=(224,224,3), pooling='avg')\n",
        "\n",
        "  model = Sequential([\n",
        "      resnet152,\n",
        "      # add hidden layers after tuning\n",
        "      Dense(512, activation=active),\n",
        "      Dropout(dropout),\n",
        "      Dense(256, activation=active),\n",
        "      Dropout(dropout),\n",
        "      Dense(4, activation='sigmoid')\n",
        "  ])\n",
        "   # Freeze the layers of the pre-trained model\n",
        "  for layer in resnet152.layers:\n",
        "      layer.trainable = False\n",
        "\n",
        "  adam = Adam(learning_rate=learning_rate)\n",
        "  model.compile(optimizer = adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "  #gridsearch on 2 activation function, 2 dropout rate and 3 learning_rate\n",
        "param_grid = dict(active=['relu', 'tanh'],learning_rate = [0.01,0.001,0.0001], dropout = [0.5,0.8])\n",
        "\n",
        "hp_model = KerasClassifier(build_fn=Grid_Search_TL_model, verbose=0)\n",
        "#instantiate gridsearch object using 3 fold crossvaliadtion\n",
        "grid = GridSearchCV(estimator=hp_model, param_grid=param_grid, cv=3,error_score='raise')\n",
        "\n",
        "# get the training and validation data generators and their corresponding targets\n",
        "X_train, y_train = train_datagen.flow_from_directory('./', subset='training', target_size=(img_width, img_height), batch_size=batch_size, class_mode='categorical').next()\n",
        "X_validate, y_validate = train_datagen.flow_from_directory('./', subset='validation', target_size=(img_width, img_height), batch_size=batch_size, class_mode='categorical').next()\n",
        "\n",
        "grid_result = grid.fit(X_train, y_train, validation_data=(X_validate, y_validate), epochs = 1, callbacks=[EarlyStopping(patience=3)])\n",
        "\n",
        "#determine the best parameter\n",
        "print(grid_result.best_params_)\n",
        "print(grid_result.best_score_)"
      ],
      "metadata": {
        "id": "zHFyvzbtcHa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet152 = ResNet152(weights = \"imagenet\", include_top = False, input_shape=(224,224,3), pooling='avg')\n",
        "\n",
        "\n",
        "# Freeze the layers of the pre-trained model\n",
        "for layer in resnet152.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# resnet152.summary()\n",
        "\n",
        "# Create the model\n",
        "model = Sequential([\n",
        "    resnet152,\n",
        "    Dense(512, activation='tanh'),\n",
        "    Dropout(0.5),\n",
        "    Dense(128, activation='tanh'),\n",
        "    Dropout(0.5),\n",
        "    Dense(4, activation='sigmoid')\n",
        "])\n",
        "adam = Adam(learning_rate=0.0001)\n",
        "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# plot of ResNet152 model\n",
        "plot_model(model, to_file='ResNet152-plant.png', show_shapes=True)"
      ],
      "metadata": {
        "id": "ZKXYb9p4XVW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pick loss, val_loss, accuracy and val_accuracy for comparsion\n",
        "hist = model.fit(train_data, validation_data = validate_data, epochs = 20, verbose = 1, batch_size=64)\n",
        "train_loss = hist.history['loss']\n",
        "val_loss = hist.history['val_loss']\n",
        "train_accuracy = hist.history['accuracy']\n",
        "val_accuracy = hist.history['val_accuracy']\n",
        "\n",
        "# print loss, val_loss, accuracy and val_accuracy\n",
        "print(train_loss)\n",
        "print(train_accuracy)\n",
        "print(val_loss)\n",
        "print(val_accuracy)\n",
        "\n",
        "# loss for each iteration, and make a plot of iterations/epochs vs loss\n",
        "epochs = list(range(1,21))\n",
        "plt.figure(figsize=(9,3))\n",
        "plt.plot(epochs,train_loss, color='blue', label='train loss')\n",
        "plt.plot(epochs,val_loss, color='orange', label='validation loss')\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.title(\"Loss vs Epoch (ResNet152)\")\n",
        "plt.legend(loc='best')\n",
        "plt.show()\n",
        "\n",
        "# accuracy for each iteration, and make a plot of iterations/epochs vs accuracy\n",
        "plt.figure(figsize=(9,3))\n",
        "plt.plot(epochs,train_accuracy, color='blue', label='train accuracy')\n",
        "plt.plot(epochs,val_accuracy, color='orange', label='validation accuracy')\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.title('Accuracy vs Epoch (ResNet152)')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "20YwzlFHXcfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print out model accuracy\n",
        "loss, accuracy = model.evaluate(test_data, verbose=0)\n",
        "print('Test accuracy: %.2f%%' % (accuracy * 100))"
      ],
      "metadata": {
        "id": "xr04dz0yY0TI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "real_value = []\n",
        "pred_value = []\n",
        "\n",
        "\n",
        "\n",
        "labels = dict()\n",
        "\n",
        "# perdict each batch of images \n",
        "for images_batch, labels_batch in test_data:\n",
        "    for i in range(images_batch.shape[0]):\n",
        "        image = images_batch[i].numpy().astype('uint8')\n",
        "        label = labels_batch[i].numpy()\n",
        "        \n",
        "        real_value.append(np.argmax(label, axis=0))\n",
        "    \n",
        "        batch_prediction = model.predict(images_batch, verbose=0)\n",
        "\n",
        "        pred_value.append(np.argmax(batch_prediction[i]))\n",
        "\n",
        "real_value1 = []\n",
        "pred_value1 = []\n",
        "\n",
        "# convert perdict reult for future scores calculate\n",
        "for i in real_value:\n",
        "  if i == 0:\n",
        "    real_value1.append([1,0,0,0])\n",
        "  elif i == 1:\n",
        "    real_value1.append([0,1,0,0])\n",
        "  elif i == 2:\n",
        "    real_value1.append([0,0,1,0])\n",
        "  elif i == 3:\n",
        "    real_value1.append([0,0,0,1])\n",
        "\n",
        "for i in pred_value:\n",
        "  if i == 0:\n",
        "    pred_value1.append([1,0,0,0])\n",
        "  elif i == 1:\n",
        "    pred_value1.append([0,1,0,0])\n",
        "  elif i == 2:\n",
        "    pred_value1.append([0,0,1,0])\n",
        "  elif i == 3:\n",
        "    pred_value1.append([0,0,0,1])\n",
        "\n",
        "# calculate precision, recall and f1 socres and print\n",
        "precision = precision_score(real_value1, pred_value1, average='macro')\n",
        "recall = recall_score(real_value1, pred_value1, average='macro')\n",
        "f1 = f1_score(real_value1, pred_value1, average='macro')\n",
        "\n",
        "print('Precision: {:.3f}'.format(precision))\n",
        "print('Recall: {:.3f}'.format(recall))\n",
        "print('F1 score: {:.3f}'.format(f1))"
      ],
      "metadata": {
        "id": "ubB7vLsPFL6R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}